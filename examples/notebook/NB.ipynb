{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration(begin, t_del=None):\n",
    "    if t_del is None:\n",
    "        t_del = datetime.datetime.fromtimestamp(time.time()) - datetime.datetime.fromtimestamp(begin)\n",
    "    return t_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 16, 6, 5, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = randn(N, D_in)\n",
    "np.random.seed(2)\n",
    "y = randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 5), (5,), (5, 2), (2,))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "w1 = randn(D_in, H) \n",
    "np.random.seed(23)\n",
    "b1 = randn(H)\n",
    "np.random.seed(4)\n",
    "w2 = randn(H, D_out)\n",
    "np.random.seed(24)\n",
    "b2 = randn(D_out)\n",
    "w1.shape, b1.shape, w2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bias = False\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(6, 5, bias=use_bias)\n",
    "        self.l2 = nn.Linear(5, 2, bias=use_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.l1(x))\n",
    "        x= self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAD(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(DAD, self).__init__()\n",
    "        self.in_activations = []\n",
    "        self.out_activations = []\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, *inputs, **kwargs):\n",
    "        childrens = dict(self.model.named_children())\n",
    "        for k, ch in childrens.items():\n",
    "            ch.register_forward_hook(self.hook_wrapper('forward', k))\n",
    "        return self.model(*inputs, **kwargs)\n",
    "    \n",
    "    def hook_wrapper(self, hook_type, layer):\n",
    "        def fw_hook(a, in_act, out_act):\n",
    "#             print(f'----IN----------------------')\n",
    "#             print(in_act)\n",
    "#             print(f'{out_act.shape}----OUT----------------------')\n",
    "#             print(out_act)\n",
    "            self.out_activations.append(out_act)\n",
    "            self.in_activations.append(in_act[0])\n",
    "            \n",
    "        return fw_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DAD(Net())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n, m), w in zip(net.named_parameters(), [w1, w2]):\n",
    "    if 'bias' in n:\n",
    "        m.data = torch.FloatTensor(w)\n",
    "    elif 'weight' in n:\n",
    "        m.data = torch.FloatTensor(w.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "o = net(torch.FloatTensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.square(o-torch.Tensor(y)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back(_optim, x, y, iters=3000):\n",
    "    tot = datetime.timedelta(0)\n",
    "    for i in range(iters):\n",
    "        net = DAD(Net())\n",
    "        _loss = torch.square(net(x)-y).sum()\n",
    "        _st = time.time()\n",
    "        _loss.backward()\n",
    "        _optim.step()\n",
    "        tot += duration(_st)\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkt = back(optim, torch.FloatTensor(x), torch.FloatTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:00:00.528007'"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{bkt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads = []\n",
    "# for op in net.out_activations:\n",
    "#     grads.append(torch.autograd.grad(loss, op, retain_graph=True)[0])\n",
    "#     print('------------')\n",
    "# grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dadback(optim, x, y, iters=100):\n",
    "    tot = datetime.timedelta(0)\n",
    "    for i in range(iters):\n",
    "        net = DAD(Net())\n",
    "        _loss = torch.square(net(x)-y).sum()\n",
    "        \n",
    "        _st = time.time()\n",
    "        grads = []\n",
    "        for op in net.out_activations:\n",
    "            grads.append(torch.autograd.grad(_loss, op, retain_graph=True)[0])\n",
    "            \n",
    "        for p, a, g in zip(net.parameters(), net.in_activations, grads):\n",
    "            p.grad = torch.FloatTensor(a.T.mm(g)).T\n",
    "            \n",
    "        optim.step()\n",
    "        tot += duration(_st)\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "dadt = dadback(optim, torch.FloatTensor(x), torch.FloatTensor(y), 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:00:00.709344'"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{dadt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: (16, 5)\n",
      "grad_y_pred: (16, 2)\n",
      "grad_w2:  (5, 2)\n",
      "grad_h:  (16, 5)\n",
      "grad_w1: (6, 5)\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    h = 1/(1+np.exp(-x.dot(w1)))\n",
    "    print('h:', h.shape)\n",
    "    y_pred = h.dot(w2)\n",
    "    loss1 = np.square(y_pred-y).sum()\n",
    "#     print(t, loss)\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    print('grad_y_pred:', grad_y_pred.shape)\n",
    "    \n",
    "    grad_w2 = h.T.dot(grad_y_pred)\n",
    "    print('grad_w2: ', grad_w2.shape)\n",
    "    \n",
    "    \n",
    "    grad_h = grad_y_pred.dot(w2.T)\n",
    "    print('grad_h: ', grad_h.shape)\n",
    "    grad_w1 = x.T.dot(grad_h * h * (1-h))\n",
    "    print('grad_w1:', grad_w1.shape)\n",
    "    \n",
    "    w1 -= 1e-4 * grad_w1\n",
    "    w2 -= 1e-4 * grad_w2\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1430, -2.9317],\n",
       "         [ 2.5737, -5.2007],\n",
       "         [ 0.8187, -0.8654],\n",
       "         [-1.7404,  2.8364],\n",
       "         [ 0.4730, -0.4396],\n",
       "         [-2.0773, -6.3996],\n",
       "         [-2.4732,  1.5807],\n",
       "         [-3.1482,  0.3821],\n",
       "         [-1.3892, -2.6533],\n",
       "         [-1.0834,  0.6593],\n",
       "         [-0.7269, -0.8889],\n",
       "         [-3.0894,  0.2824],\n",
       "         [-1.2846, -2.5343],\n",
       "         [-0.1636,  1.5470],\n",
       "         [ 1.7658,  0.3033],\n",
       "         [-0.9892, -6.1566]]),\n",
       " tensor([[-1.1260e-02, -4.1599e-01,  1.4838e-01, -7.6555e-02,  7.4904e-01],\n",
       "         [-1.3344e-01, -1.5005e+00,  1.5068e-01, -9.4621e-01,  1.4238e+00],\n",
       "         [-8.8707e-02, -2.8083e-01,  5.5806e-02, -1.0102e-01,  1.4115e-01],\n",
       "         [ 3.0874e-01,  5.5254e-01, -4.4421e-01,  5.4637e-01, -2.3730e-01],\n",
       "         [-1.5892e-02, -1.3514e-01,  8.0408e-02, -2.0471e-02,  1.1434e-01],\n",
       "         [-6.6903e-01, -5.8514e-01,  2.7372e+00, -5.2326e-01,  7.1344e-01],\n",
       "         [ 1.5730e-02,  8.8936e-01, -3.6619e-01,  4.7740e-03, -6.5570e-01],\n",
       "         [ 9.0980e-04,  6.0522e-01,  1.7625e-01,  4.8553e-01, -3.4322e-01],\n",
       "         [-2.3060e-01, -1.1005e-01,  8.4526e-01, -1.7271e-01,  4.2672e-01],\n",
       "         [ 1.2154e-02,  3.5271e-01, -1.2605e-01,  1.0658e-01, -1.4589e-01],\n",
       "         [-2.4594e-02,  2.5999e-02,  3.6646e-01, -6.0715e-03,  1.8129e-01],\n",
       "         [-2.7960e-03,  1.4174e-01,  1.5188e-01,  5.3409e-01, -1.2150e-01],\n",
       "         [-3.3383e-01, -5.2667e-02,  3.8204e-01, -4.8522e-02,  2.7918e-01],\n",
       "         [ 1.9281e-01,  3.0765e-01, -5.9186e-01,  1.7270e-01, -4.2857e-01],\n",
       "         [ 6.0736e-02, -3.8592e-01, -2.1284e-01, -1.0146e-01,  4.8860e-02],\n",
       "         [-2.6688e-01, -7.7818e-01,  1.7342e+00, -8.9245e-02,  1.6637e+00]])]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = []\n",
    "for op in net.out_activations[::-1]:\n",
    "    grads.append(torch.autograd.grad(loss, op, retain_graph=True)[0])\n",
    "    print('------------')\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1260e-02, -4.1599e-01,  1.4838e-01, -7.6555e-02,  7.4904e-01],\n",
       "        [-1.3344e-01, -1.5005e+00,  1.5068e-01, -9.4621e-01,  1.4238e+00],\n",
       "        [-8.8707e-02, -2.8083e-01,  5.5806e-02, -1.0102e-01,  1.4115e-01],\n",
       "        [ 3.0874e-01,  5.5254e-01, -4.4421e-01,  5.4637e-01, -2.3730e-01],\n",
       "        [-1.5892e-02, -1.3514e-01,  8.0408e-02, -2.0471e-02,  1.1434e-01],\n",
       "        [-6.6903e-01, -5.8514e-01,  2.7372e+00, -5.2326e-01,  7.1344e-01],\n",
       "        [ 1.5730e-02,  8.8936e-01, -3.6619e-01,  4.7740e-03, -6.5570e-01],\n",
       "        [ 9.0980e-04,  6.0522e-01,  1.7625e-01,  4.8553e-01, -3.4322e-01],\n",
       "        [-2.3060e-01, -1.1005e-01,  8.4526e-01, -1.7271e-01,  4.2672e-01],\n",
       "        [ 1.2154e-02,  3.5271e-01, -1.2605e-01,  1.0658e-01, -1.4589e-01],\n",
       "        [-2.4594e-02,  2.5999e-02,  3.6646e-01, -6.0715e-03,  1.8129e-01],\n",
       "        [-2.7960e-03,  1.4174e-01,  1.5188e-01,  5.3409e-01, -1.2150e-01],\n",
       "        [-3.3383e-01, -5.2667e-02,  3.8204e-01, -4.8522e-02,  2.7918e-01],\n",
       "        [ 1.9281e-01,  3.0765e-01, -5.9186e-01,  1.7270e-01, -4.2857e-01],\n",
       "        [ 6.0736e-02, -3.8592e-01, -2.1284e-01, -1.0146e-01,  4.8860e-02],\n",
       "        [-2.6688e-01, -7.7818e-01,  1.7342e+00, -8.9245e-02,  1.6637e+00]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(loss, net.out_activations, retain_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 4.8665e+00, -7.3381e-01,  3.3880e+00, -2.9801e+00,  6.3372e-01],\n",
       "         [ 2.8089e+00, -3.4093e-01,  3.8159e+00, -9.8507e-01, -8.5849e-01],\n",
       "         [-6.5411e-01,  9.8146e-01,  2.7933e+00,  2.1102e+00,  1.9178e+00],\n",
       "         [-5.7586e-01, -1.4998e+00, -1.8408e+00, -1.0347e+00, -2.6430e+00],\n",
       "         [ 2.3337e+00,  1.2396e+00,  1.3698e+00, -3.2489e+00,  1.2513e+00],\n",
       "         [ 9.5344e-01, -2.8579e-01, -1.1350e-01, -8.4780e-01,  1.9675e+00],\n",
       "         [-3.7026e+00,  3.9338e-02, -8.0749e-02,  6.2752e+00, -1.3347e-01],\n",
       "         [-3.4696e+00,  1.1964e+00, -1.5698e-01, -7.9277e-01, -5.7189e-01],\n",
       "         [ 1.3475e+00,  4.7524e-01, -1.2089e+00, -8.6037e-02,  1.3252e+00],\n",
       "         [-3.0309e+00,  5.8706e-01, -8.0442e-01,  2.0985e+00, -1.6988e+00],\n",
       "         [-2.8721e+00,  3.4168e-02,  7.9301e-01,  2.1079e+00, -5.1637e-01],\n",
       "         [ 1.2504e+00,  3.0449e+00,  1.1749e+00,  2.3448e-01,  2.2014e+00],\n",
       "         [ 1.5234e-01,  1.9598e+00,  2.2812e+00, -2.4935e+00,  1.9025e+00],\n",
       "         [ 1.1007e-03,  1.6776e-01, -1.2981e-01, -1.3111e+00, -5.0271e-01],\n",
       "         [ 9.0855e-02, -4.2261e-02, -1.2302e+00, -1.9906e+00, -9.4464e-01],\n",
       "         [ 2.2735e+00,  4.8739e-01,  1.2727e+00, -3.4702e+00, -1.8993e-01]],\n",
       "        grad_fn=<MmBackward>),\n",
       " tensor([[-0.4883, -1.5221],\n",
       "         [-0.8493, -0.9601],\n",
       "         [-1.3841, -1.2745],\n",
       "         [-0.3673,  0.1729],\n",
       "         [-0.8214, -1.1288],\n",
       "         [-0.4872, -0.9076],\n",
       "         [-1.1951, -0.3276],\n",
       "         [-1.0350, -0.4051],\n",
       "         [-0.7137, -0.1517],\n",
       "         [-1.2896,  0.3387],\n",
       "         [-1.2416, -0.6009],\n",
       "         [-1.2881, -0.8476],\n",
       "         [-0.9811, -1.5033],\n",
       "         [-0.7195, -0.4141],\n",
       "         [-0.5383, -0.0018],\n",
       "         [-0.7637, -0.8469]], grad_fn=<MmBackward>)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.out_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dadback(net, grads, optim):\n",
    "    for p, (g, act) in zip(net.parameters(), grads):\n",
    "        p.grad = torch.FloatTensor(act.T.mm(g)).T\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.28 ms, sys: 408 Âµs, total: 4.69 ms\n",
      "Wall time: 2.58 ms\n"
     ]
    }
   ],
   "source": [
    "%time dadback(net, grads, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.6243, -0.6118, -0.5282, -1.0730,  0.8654, -2.3015],\n",
       "         [ 1.7448, -0.7612,  0.3190, -0.2494,  1.4621, -2.0601],\n",
       "         [-0.3224, -0.3841,  1.1338, -1.0999, -0.1724, -0.8779],\n",
       "         [ 0.0422,  0.5828, -1.1006,  1.1447,  0.9016,  0.5025],\n",
       "         [ 0.9009, -0.6837, -0.1229, -0.9358, -0.2679,  0.5304],\n",
       "         [-0.6917, -0.3968, -0.6872, -0.8452, -0.6712, -0.0127],\n",
       "         [-1.1173,  0.2344,  1.6598,  0.7420, -0.1918, -0.8876],\n",
       "         [-0.7472,  1.6925,  0.0508, -0.6370,  0.1909,  2.1003],\n",
       "         [ 0.1202,  0.6172,  0.3002, -0.3522, -1.1425, -0.3493],\n",
       "         [-0.2089,  0.5866,  0.8390,  0.9311,  0.2856,  0.8851],\n",
       "         [-0.7544,  1.2529,  0.5129, -0.2981,  0.4885, -0.0756],\n",
       "         [ 1.1316,  1.5198,  2.1856, -1.3965, -1.4441, -0.5045],\n",
       "         [ 0.1600,  0.8762,  0.3156, -2.0222, -0.3062,  0.8280],\n",
       "         [ 0.2301,  0.7620, -0.2223, -0.2008,  0.1866,  0.4101],\n",
       "         [ 0.1983,  0.1190, -0.6707,  0.3776,  0.1218,  1.1295],\n",
       "         [ 1.1989,  0.1852, -0.3753, -0.6387,  0.4235,  0.0773]]),\n",
       " tensor([[0.9924, 0.3262, 0.9672, 0.0485, 0.6513],\n",
       "         [0.9432, 0.4174, 0.9785, 0.2726, 0.2960],\n",
       "         [0.3423, 0.7277, 0.9422, 0.8920, 0.8717],\n",
       "         [0.3596, 0.1823, 0.1372, 0.2620, 0.0665],\n",
       "         [0.9117, 0.7759, 0.7970, 0.0374, 0.7771],\n",
       "         [0.7219, 0.4291, 0.4711, 0.2999, 0.8773],\n",
       "         [0.0241, 0.5092, 0.4803, 0.9981, 0.4674],\n",
       "         [0.0302, 0.7669, 0.4608, 0.3108, 0.3619],\n",
       "         [0.7938, 0.6165, 0.2296, 0.4784, 0.7901],\n",
       "         [0.0460, 0.6419, 0.3096, 0.8906, 0.1551],\n",
       "         [0.0535, 0.5080, 0.6885, 0.8916, 0.3741],\n",
       "         [0.7776, 0.9546, 0.7635, 0.5581, 0.9003],\n",
       "         [0.5383, 0.8765, 0.9070, 0.0762, 0.8701],\n",
       "         [0.5003, 0.5417, 0.4675, 0.2122, 0.3770],\n",
       "         [0.5226, 0.4891, 0.2263, 0.1201, 0.2803],\n",
       "         [0.9067, 0.6201, 0.7810, 0.0302, 0.4519]], grad_fn=<SigmoidBackward>)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.in_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(_):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "a(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model',\n",
       "  Net(\n",
       "    (l1): Linear(in_features=6, out_features=5, bias=True)\n",
       "    (l2): Linear(in_features=5, out_features=2, bias=True)\n",
       "  ))]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = list(net.model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
