{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "import numpy as np\n",
    "import os\n",
    "from dad_torch.distrib import DadHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 16, 6, 5, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = N//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = randn(N, D_in)\n",
    "np.random.seed(2)\n",
    "y = randn(N, D_out)\n",
    "\n",
    "x1, x2 = x[:mid], x[mid:]\n",
    "y1, y2 = y[:mid], y[mid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 5), (5, 2))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "w1 = randn(D_in, H) \n",
    "np.random.seed(4)\n",
    "w2 = randn(H, D_out)\n",
    "w1.shape, w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(6, 5, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(5)\n",
    "        self.l2 = nn.Linear(5, 2, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.l1(x))\n",
    "        x = self.bn(x)\n",
    "        x= self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DadHook(\n",
       "  (module): Net(\n",
       "    (l1): Linear(in_features=6, out_features=5, bias=False)\n",
       "    (bn): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (l2): Linear(in_features=5, out_features=2, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DadHook(Net())\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook_wrapper(site, hook_type, layer, save_to='', debug=False):\n",
    "#     if debug:\n",
    "#         print(f\"**** {site}, {hook_type}, {layer} ****\")\n",
    "\n",
    "#     name = os.path.join(save_to, f\"Site:{site}-Type:{hook_type}-Layer:{layer}\")\n",
    "\n",
    "#     def hook_save(a, in_grad, out_grad):\n",
    "#         if hook_type.lower() == 'forward':\n",
    "#             for i, b in enumerate(in_grad):\n",
    "#                 if b is not None:\n",
    "#                     np.save(name + f\"-IO:in-Index:{i}.npy\", b.clone().detach().numpy())\n",
    "#                 break\n",
    "#         if hook_type.lower() == 'backward':\n",
    "#             for i, c in enumerate(out_grad):\n",
    "#                 if c is not None:\n",
    "#                     np.save(name + f\"-IO:out-index:{i}.npy\", c.clone().detach().numpy())\n",
    "#                 break\n",
    "\n",
    "#     return hook_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: (16, 5)\n",
      "grad_y_pred: (16, 2)\n",
      "grad_w2:  (5, 2)\n",
      "grad_h:  (16, 5)\n",
      "grad_w1: (6, 5)\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    h = 1/(1+np.exp(-x.dot(w1)))\n",
    "    print('h:', h.shape)\n",
    "    y_pred = h.dot(w2)\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "#     print(t, loss)\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    print('grad_y_pred:', grad_y_pred.shape)\n",
    "    \n",
    "    grad_w2 = h.T.dot(grad_y_pred)\n",
    "    print('grad_w2: ', grad_w2.shape)\n",
    "    \n",
    "    \n",
    "    grad_h = grad_y_pred.dot(w2.T)\n",
    "    print('grad_h: ', grad_h.shape)\n",
    "    grad_w1 = x.T.dot(grad_h * h * (1-h))\n",
    "    print('grad_w1:', grad_w1.shape)\n",
    "    \n",
    "    w1 -= 1e-4 * grad_w1\n",
    "    w2 -= 1e-4 * grad_w2\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "p1 = net(torch.Tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.square(p1-torch.Tensor(y)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = net._activations['bn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = net._local_grads['bn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 5]), torch.Size([16, 5]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act.shape, grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = list(net.module.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for c in ch:\n",
    "    print(isinstance(c, nn.BatchNorm1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict([('weight', Parameter containing:\n",
       "               tensor([1., 1., 1., 1., 1.], requires_grad=True)),\n",
       "              ('bias',\n",
       "               Parameter containing:\n",
       "               tensor([0., 0., 0., 0., 0.], requires_grad=True))]),\n",
       " '_buffers': OrderedDict([('running_mean',\n",
       "               tensor([0.0472, 0.0526, 0.0464, 0.0501, 0.0480])),\n",
       "              ('running_var',\n",
       "               tensor([0.9014, 0.9002, 0.9022, 0.9021, 0.9016])),\n",
       "              ('num_batches_tracked', tensor(1))]),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict([(36,\n",
       "               <function dad_torch.distrib.utils.DadHook._hook_fn.<locals>.get(m, in_grad, out_grad)>)]),\n",
       " '_is_full_backward_hook': False,\n",
       " '_forward_hooks': OrderedDict([(35,\n",
       "               <function dad_torch.distrib.utils.DadHook._hook_fn.<locals>.get(m, in_grad, out_grad)>)]),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'num_features': 5,\n",
       " 'eps': 1e-05,\n",
       " 'momentum': 0.1,\n",
       " 'affine': True,\n",
       " 'track_running_stats': True}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(ch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.1606,  2.1797,  1.8189, -0.2858,  5.2030])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[1][1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta =  params[2][1].grad.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 5]), torch.Size([16, 5]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape, act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.1682, 11.4447,  3.0139,  9.8893, -3.6210], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(act.T.mm(grad)).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = (grad * act).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.weight Parameter containing:\n",
      "tensor([[ 1.7886, -0.3548, -1.3139, -0.4047, -1.1850, -0.7130],\n",
      "        [ 0.4365, -0.0827,  0.8846, -0.5454, -0.2056,  0.6252],\n",
      "        [ 0.0965, -0.6270,  0.8813, -1.5465,  1.4861, -0.1605],\n",
      "        [-1.8635, -0.0438,  1.7096,  0.9824,  0.2367, -0.7688],\n",
      "        [-0.2774, -0.4772,  0.0500, -1.1011, -1.0238, -0.2300]],\n",
      "       requires_grad=True)\n",
      "l2.weight Parameter containing:\n",
      "tensor([[ 0.0506, -0.9959, -0.4183, -0.6477,  0.3323],\n",
      "        [ 0.5000,  0.6936, -1.5846,  0.5986, -1.1475]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k, p in net.named_parameters():\n",
    "    print(k,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in net.parameters():\n",
    "    p.grad.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.load_state_dict(torch.load('chk.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: AK'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Name: {}\".format('AK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.T.mm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.65050639, -20.05829342],\n",
       "       [ -8.77522329, -11.72429864],\n",
       "       [ -5.49211195, -17.99616749],\n",
       "       [ -7.0682281 ,  -3.19438619],\n",
       "       [ -7.95513326, -15.59246294]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.T.dot(grad_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.6505, -20.0583],\n",
       "        [ -8.7752, -11.7243],\n",
       "        [ -5.4921, -17.9962],\n",
       "        [ -7.0682,  -3.1944],\n",
       "        [ -7.9551, -15.5925]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(h).T.mm(torch.Tensor(grad_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 6]), torch.Size([2, 5])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.grad.shape for p in net.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.weight tensor([[-0.1147,  0.2385, -0.1600,  2.0195,  0.8291,  0.4634],\n",
      "        [-5.2419,  3.7720,  1.7932,  3.2138, -1.9054,  4.8540],\n",
      "        [ 0.9208, -0.0698, -1.5408, -6.1658, -2.3991, -0.7603],\n",
      "        [-1.2793,  2.9918,  0.6647,  0.7110, -1.0412,  3.3018],\n",
      "        [ 6.0451, -2.3750, -1.7649, -4.2975,  2.3372, -5.0113]])\n",
      "l2.weight tensor([[ -3.6777,  -8.8142,  -5.5268,  -7.0952,  -7.9853],\n",
      "        [-20.1403, -11.7940, -18.0874,  -3.2346, -15.6785]])\n"
     ]
    }
   ],
   "source": [
    "for l, ch in net.named_parameters():\n",
    "    print(l, ch.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _hierarchy_key(*args):\n",
    "    return \".\".join([f\"{a}\" for a in args])\n",
    "\n",
    "\n",
    "class DADParallel(torch.nn.Module):\n",
    "    def __init__(self, module, device=None, reduction_method='dad', reduction_rank=5, num_pow_iters=1, **kw):\n",
    "#         assert _dist.is_initialized(), \"*** Default process group is not initialized. ***\"\n",
    "\n",
    "        super(DADParallel, self).__init__()\n",
    "        self.module = module\n",
    "        self.device = device\n",
    "        self.cache = kw.get('cache', {})\n",
    "        self.reduction_method = reduction_method\n",
    "        self.reduction_rank = reduction_rank\n",
    "        self.num_pow_iters = num_pow_iters\n",
    "        self.commn_mode = kw.get('commn_mode', 'all_gather')\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.fw_hooks_handle = []\n",
    "        self.bk_hooks_handle = []\n",
    "        self._activations = {}\n",
    "        self._local_grads = {}\n",
    "\n",
    "    def _hook_fn(self, hook_type, hook_key):\n",
    "        def get(m, in_grad, out_grad):\n",
    "            if hook_type.lower() == 'forward':\n",
    "                for i, b in enumerate(in_grad):\n",
    "                    if b is not None:\n",
    "                        self._activations[hook_key] = b\n",
    "                    break\n",
    "            if hook_type.lower() == 'backward':\n",
    "                for i, c in enumerate(out_grad):\n",
    "                    if c is not None:\n",
    "                        self._local_grads[hook_key] = c\n",
    "                    break\n",
    "\n",
    "        return get\n",
    "\n",
    "    def _hook(self):\n",
    "        def _hook_recursive(module_name, module):\n",
    "            children = list(module.named_children())[::-1]\n",
    "            if len(children) > 0:\n",
    "                for children_name, child in children:\n",
    "                    _hook_recursive(_hierarchy_key(module_name, children_name), child)\n",
    "            elif len(list(module.parameters())) > 0:\n",
    "                self.fw_hooks_handle.append(\n",
    "                    module.register_forward_hook(self._hook_fn('forward', module_name))\n",
    "                )\n",
    "                self.bk_hooks_handle.append(\n",
    "                    module.register_backward_hook(self._hook_fn('backward', module_name))\n",
    "                )\n",
    "\n",
    "        if self.training:\n",
    "            for ch_name, ch in list(self.module.named_children())[::-1]:\n",
    "                _hook_recursive(ch_name, ch)\n",
    "\n",
    "    def _unhook(self):\n",
    "        for hk in self.fw_hooks_handle:\n",
    "            hk.remove()\n",
    "        for hk in self.bk_hooks_handle:\n",
    "            hk.remove()\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        self.module.train(mode)\n",
    "        self._hook()\n",
    "        return self\n",
    "\n",
    "    def eval(self):\n",
    "        self.module.eval()\n",
    "        self._unhook()\n",
    "        return self\n",
    "\n",
    "    def forward(self, *inputs, **kwargs):\n",
    "        if self.training:\n",
    "            self._reset()\n",
    "        output = self.module(*inputs, **kwargs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lo = nn.Linear(64, 32)\n",
    "        self.decoder = nn.Sequential(nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 6), nn.ReLU())\n",
    "        self.l1 = nn.Linear(6, 5, bias=False)\n",
    "        self.l2 = nn.Linear(5, 2, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.lo(x))\n",
    "        x = self.decoder(x)\n",
    "        x= torch.sigmoid(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Net()\n",
    "dnet = DADParallel(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DADParallel(\n",
       "  (module): Net(\n",
       "    (lo): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=16, out_features=6, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (l1): Linear(in_features=6, out_features=5, bias=False)\n",
       "    (l2): Linear(in_features=5, out_features=2, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "o = dnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lo': tensor([[-0.4835, -1.1069, -0.7140, -0.8197, -0.0525,  0.1120,  1.7470,  1.1938,\n",
       "           0.1551, -0.6305,  0.6991, -0.4405,  0.6725, -1.8085,  0.5469, -0.8451,\n",
       "          -2.0803, -0.7865, -0.3121, -0.8956,  1.6029,  0.5956, -0.4439,  0.9837,\n",
       "          -0.7879,  2.0132, -1.3898, -0.1196, -0.4305,  0.3405,  0.0573,  1.7856,\n",
       "           0.7943,  0.0197,  1.3671, -1.8850,  0.7181, -0.3402, -0.5229, -0.1278,\n",
       "          -1.3658, -0.9323, -1.4480,  0.3277, -0.4665, -1.0870,  0.9598, -0.5879,\n",
       "           0.8310,  0.8189,  0.7159, -1.8115,  0.0307,  1.1331,  0.6225,  0.6362,\n",
       "           0.2708, -0.0050, -0.9191,  1.0267, -0.0897,  1.7244, -1.7051, -0.9401],\n",
       "         [-0.4094, -0.7636, -0.1106,  1.0749,  0.2856,  1.5373,  0.1219, -0.1334,\n",
       "          -1.8924, -1.1252, -0.8243, -0.8567, -0.4680, -1.0060,  1.4966, -0.9700,\n",
       "          -1.3467,  0.3709,  1.9041, -0.1403, -0.2137, -1.8996, -1.3642,  1.5709,\n",
       "           0.6732,  0.5717, -0.2123, -0.7953, -0.3978,  1.2180, -1.5668,  0.6420,\n",
       "           0.9321, -0.7944,  0.0558,  0.4758, -0.0324, -0.4772,  0.1610, -1.6645,\n",
       "           1.1036, -0.5866, -0.4254,  0.1684,  1.0909,  1.1924, -0.6226,  0.7058,\n",
       "          -0.6659,  1.8322,  0.1041,  1.5178,  0.0071,  2.0406, -0.4270, -0.0777,\n",
       "           0.1717,  0.1762, -0.3375, -1.2000,  0.5444,  1.2433, -0.6373,  0.5434],\n",
       "         [-0.6117,  0.0281, -0.8504, -1.5669,  0.7363, -1.5937,  1.2661,  1.8585,\n",
       "          -0.1408, -0.2274, -0.5230,  0.1844,  0.0046,  0.5773,  0.4097,  0.3149,\n",
       "           0.8233, -0.3471, -0.4297, -0.2878,  0.4895,  1.5135, -0.3482,  0.0118,\n",
       "           0.2602,  0.5979,  0.4986, -0.1391, -0.6283,  0.4505, -0.1567, -0.0748,\n",
       "          -1.3045, -0.4773, -1.7458, -0.8902, -0.8870, -0.8522,  0.6350, -0.1493,\n",
       "          -0.3603, -0.4280, -0.6141,  0.8403, -0.4278, -0.4474, -0.6647,  0.9099,\n",
       "          -1.1774,  1.1115, -1.4152, -0.4340,  0.3867, -0.0822, -2.1376, -0.7757,\n",
       "           0.4336,  0.2909,  0.0644,  0.6739, -0.2310, -0.1873, -1.1536, -0.3904],\n",
       "         [-0.8170, -0.7548,  1.0212, -0.6794,  0.5447, -1.2104,  1.2590, -0.0105,\n",
       "           1.9407, -0.1020,  0.8955,  0.4653, -1.6181,  1.1100,  0.1596, -1.4906,\n",
       "          -0.3461, -1.0468, -0.7649, -0.0976, -0.7792,  0.1696,  0.0698,  0.4056,\n",
       "          -1.4579, -0.8830, -0.1684,  0.3307,  0.3282,  1.0234,  0.4654, -0.6546,\n",
       "           0.7812, -1.6520,  2.0812, -0.2223,  1.0535, -0.7658, -0.1823, -0.5631,\n",
       "           1.3401,  2.1166,  0.7767, -1.0922, -1.6113,  0.7260, -0.3975,  0.4112,\n",
       "          -0.7618, -0.0539, -1.2287, -1.9895, -1.1900, -2.0400,  1.1482,  1.2040,\n",
       "           0.0717, -1.2424,  0.5215, -0.5553,  1.3425,  0.1387,  1.3497, -0.1057]]),\n",
       " 'decoder.0': tensor([[0.4709, 0.4294, 0.4639, 0.5229, 0.4003, 0.6134, 0.4158, 0.6074, 0.6200,\n",
       "          0.5205, 0.6924, 0.5717, 0.4825, 0.5904, 0.3595, 0.4382, 0.7031, 0.5618,\n",
       "          0.7315, 0.5844, 0.4272, 0.4991, 0.4603, 0.4836, 0.2412, 0.5654, 0.5017,\n",
       "          0.5250, 0.5534, 0.4761, 0.5567, 0.4647],\n",
       "         [0.4204, 0.5034, 0.5270, 0.5828, 0.3446, 0.5924, 0.5205, 0.4501, 0.3623,\n",
       "          0.4792, 0.4933, 0.5155, 0.6666, 0.4655, 0.3392, 0.5758, 0.7557, 0.5887,\n",
       "          0.7195, 0.1992, 0.4071, 0.6797, 0.6439, 0.5789, 0.5545, 0.2187, 0.7455,\n",
       "          0.2436, 0.5003, 0.5398, 0.4953, 0.6034],\n",
       "         [0.6611, 0.5701, 0.6172, 0.3537, 0.5079, 0.6533, 0.6207, 0.4785, 0.5393,\n",
       "          0.4459, 0.5258, 0.4820, 0.5638, 0.4032, 0.6138, 0.5534, 0.4985, 0.5617,\n",
       "          0.5802, 0.4361, 0.4660, 0.4010, 0.5502, 0.3313, 0.6619, 0.5503, 0.4539,\n",
       "          0.4529, 0.4322, 0.6734, 0.4825, 0.6066],\n",
       "         [0.5207, 0.4808, 0.5603, 0.6170, 0.2242, 0.5692, 0.5410, 0.5803, 0.3963,\n",
       "          0.4716, 0.6821, 0.2818, 0.4028, 0.4228, 0.5836, 0.4574, 0.5635, 0.4560,\n",
       "          0.3869, 0.4486, 0.5790, 0.3577, 0.4868, 0.7980, 0.3754, 0.6163, 0.5930,\n",
       "          0.6023, 0.2466, 0.5147, 0.7229, 0.3951]], grad_fn=<SigmoidBackward>),\n",
       " 'decoder.2': tensor([[0.0000, 0.0758, 0.1194, 0.1911, 0.0000, 0.2282, 0.0252, 0.0195, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0022],\n",
       "         [0.0000, 0.0000, 0.2012, 0.1496, 0.0000, 0.2174, 0.0000, 0.0318, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1483],\n",
       "         [0.0000, 0.0000, 0.1572, 0.1467, 0.0000, 0.1447, 0.0000, 0.2144, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2161],\n",
       "         [0.0000, 0.0386, 0.0377, 0.1677, 0.0000, 0.1133, 0.0567, 0.0232, 0.0000,\n",
       "          0.1090, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0042]],\n",
       "        grad_fn=<ReluBackward0>),\n",
       " 'l1': tensor([[0.0096, 0.0000, 0.0000, 0.1147, 0.0000, 0.0000],\n",
       "         [0.0575, 0.0000, 0.0218, 0.1635, 0.0000, 0.0000],\n",
       "         [0.0594, 0.0000, 0.0395, 0.1620, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0284, 0.1037, 0.0000, 0.0000]],\n",
       "        grad_fn=<ReluBackward0>),\n",
       " 'l2': tensor([[0.4976, 0.5079, 0.4945, 0.4955, 0.5033],\n",
       "         [0.4969, 0.5114, 0.4874, 0.4935, 0.5058],\n",
       "         [0.4958, 0.5125, 0.4862, 0.4919, 0.5061],\n",
       "         [0.4954, 0.5096, 0.4939, 0.4927, 0.5033]], grad_fn=<SigmoidBackward>)}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnet._activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(100, 50), nn.ReLU(), nn.Linear(50, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': Linear(in_features=100, out_features=50, bias=True),\n",
       " '1': ReLU(),\n",
       " '2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(net.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lo', Linear(in_features=64, out_features=32, bias=True)),\n",
       " ('decoder',\n",
       "  Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=6, bias=True)\n",
       "    (3): ReLU()\n",
       "  )),\n",
       " ('l1', Linear(in_features=6, out_features=5, bias=False)),\n",
       " ('l2', Linear(in_features=5, out_features=2, bias=False))]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dnet.module.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
